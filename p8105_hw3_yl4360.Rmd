---
title: "p8105_HW3_yl4360"
output: github_document
author: Yiyang Liu
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

library(p8105.datasets)
library(tidyverse)
library(patchwork)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

```{r message=FALSE}
data("instacart")
data("brfss_smart2010")
```

# Problem 1

## Short discription

* **The size and structure of the data:** there are `r nrow(instacart)` observations and `r ncol(instacart)` variables in the data. 11 variables are integer and 4 of them are character.

* **Describing some key variables:** 
  + `reordered`: 1 if this prodcut has been ordered by this user in the past, 0 otherwise
  + `order_dow`: the day of the week on which the order was placed
  + `eval_set`: which evaluation set this order belongs in _(Note that the data for use in this class is exclusively from the “train” `eval_set`)_

* **Giving illustrative examples of observations:** For the 1st observation, order id is `r instacart[1,1]`.  It was ordered at 10 on Thursday and it is reordered 9 days after the prior order. The product name is `r instacart[1,11]` from department `r instacart[1,15]`. The 1st order consists of 8 products. User `r instacart[1,5]` ordered them.

```{r}
aisle_level = 
  pull(instacart, aisle) %>% 
  as.factor() %>% 
  levels()
head(aisle_level)
  
aisle_max = 
  instacart %>%
  group_by(aisle) %>%
  summarise(number = n()) %>%
  filter(number == max(number)) %>%
  select(aisle) %>%
  as.character()
```

* **The number of aisles and the aisle that the most items are ordered from**: there are 134 aisles and the aisle that the most items are ordered from is `r aisle_max`.

## Plot 1 

Showing the number of items ordered in each aisle.

```{r echo=FALSE}
instacart %>% 
  group_by(aisle) %>% 
  summarise(number = n()) %>% 
  filter(number > 10000) %>% 
  ggplot(aes(x = reorder(aisle, -number), y = number))+
  geom_col(fill = "pink") +
  labs(
    title = "The number of items ordered in each aisle (n>10000)",
    x = "aisles",
    y = "number"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5),
    axis.text.x = element_text(angle = 60, hjust = 1))

```

According to the plot, it is hard to tell whether the orders from fresh vegetables or the ones from fresh fruits is the most but the result from coding is that  the orders from fresh vegetables is the most. Therefore I want to further verify.

```{r}
aisle_max_ver = 
  instacart %>% 
  group_by(aisle) %>% 
  summarise(number = n()) %>% 
  arrange(desc(number)) %>% 
  head(2)
aisle_max_ver
```

It is verified that the orders from fresh vegetables is the most.

## Table 1

Showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r}
table1 = 
  instacart %>% 
  filter(aisle == "baking ingredients" | 
         aisle == "dog food care" | 
         aisle == "packaged vegetables fruits") %>%
  group_by(aisle, product_name) %>%
  summarize(n = n()) %>%
  head(3) %>%
  arrange(desc(n))
knitr::kable (table1)
```

## Table 2

Showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r}
instacart %>%
  select(product_name, order_hour_of_day, order_dow) %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  group_by(product_name, order_dow) %>%
  summarise(mean_hour = mean(order_hour_of_day)) %>%
  mutate(
    mean_hour = round(mean_hour,1),
    order_dow = recode(order_dow, "0" = "Sun", "1" = "Mon", "2" = "Tue", "3" = "Wed", "4" = "Thur", "5" = "Fri", "6" = "Sat")
  ) %>%
  pivot_wider(
    names_from = "order_dow",
    values_from = "mean_hour"
  ) %>%  
  knitr::kable()
```

# Problem 2

* **Clean the data.**

```{r}
brfss_clean = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename("state" = "locationabbr",
       "county" = "locationdesc") %>% 
  filter(
    topic == "Overall Health",
    response == "Excellent" | 
    response == "Very good" | 
    response == "Good"| 
    response == "Fair"|
    response == "Poor") %>% 
  mutate(response = factor(response, levels = c("Poor","Fair","Good","Very good", "Excellent"))) %>% 
   arrange(response)

brfss_clean
```

* **In 2002, which states were observed at 7 or more locations? What about in 2010?**

```{r}
states2002 = 
  brfss_clean %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n_location = n_distinct(county)) %>% 
  filter(n_location >= 7) 

states2010 = 
  brfss_clean %>% 
  filter(year == 2010) %>% 
  group_by(state) %>% 
  summarize(n_location = n_distinct(county)) %>% 
  filter(n_location >= 7) 
  
```

In 2002, `r pull(states2002, state)` were observed at 7 or more locations. In 2010, `r pull(states2010, state)` were observed at 7 or more locations.

## Construct a dataset and Make a “spaghetti” plot

Dataset: limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state.

Plot: showing this average value over time within a state.

```{r}
sp_plot = 
  brfss_clean %>% 
  filter(response == "Excellent") %>% 
  select(year, state, data_value) %>% 
  group_by(state, year) %>% 
  summarise(average = mean(data_value)) %>% 
  ggplot(aes(x = year, y= average, group = state))+
  geom_line(aes(color = state))+
  labs(
      title = "Spaghetti plot of average value over time",
      x = "Year",
      y = "Average data value"
    ) +
    theme(plot.title = element_text(hjust = 0.5))

sp_plot
```

The Spaghetti plot is very complicated but we can read that average data value of most state is around 20~25 over 8 years. There is one state reached the lowest value in 2005.

## Make a two-panel plot

Showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

```{r}
tp_plot = 
  brfss_clean %>% 
  filter(
    year == 2006 | year == 2010,
    response == "Excellent" | 
    response == "Very good" | 
    response == "Good"| 
    response == "Fair"|
    response == "Poor",
    state == "NY") %>% 
  select(year, county, response, data_value) %>% 
  ggplot(aes(x = response, y = data_value, group = county, color = response))+
  geom_point(size = 3, alpha = .5) + 
  geom_line() +
  labs(
    title = "Distribution of data_value for responses in NY, 2006",
    x = "county",
    y = "data value"
  )+
  facet_grid(~year) + 
   viridis::scale_color_viridis(
    name = "County",
    discrete = TRUE) +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 60, hjust = 1))

tp_plot  
```

According to the two-panel plot, we can see that there are two more counties, Bronx and Erie added into the data in 2010. Generally, "poor" reponse matches with lowest data value and "Excellent" reponse match with highest data value. 

# Problem 3

* **Load, tidy, and otherwise wrangle the data.** Your final dataset should include all originally observed variables and values; have useful variable names; include a weekday vs weekend variable; and encode data with reasonable variable classes. Describe the resulting dataset (e.g. what variables exist, how many observations, etc).

```{r}
accel_data = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
  activity_1:activity_1440,
  names_to = "activity",
  names_prefix = "activity_",
  values_to = "activity_counts"
  ) 

weekend =
  accel_data %>%
  filter(day == "Sunday" | day == "Saturday") %>%
  mutate( weekday_weekend = "weekend") %>%
  select(week, day_id, day, weekday_weekend, everything())

weekday = 
  accel_data %>%
  filter(day == "Monday" | day == "Tuesday" | day == "Wednesday" | day == "Thursday" | day == "Friday") %>%
  mutate( weekday_weekend = "weekday") %>%
  select(week, day_id, day, weekday_weekend, everything())

accel =
  merge(weekend, weekday, all = TRUE) %>%
  arrange(week, day_id) 
```

There are `r nrow(accel_data)` observations of `r ncol(accel_data)` variables in the dataset. The variables are `r colnames(accel_data)`.

* **Traditional analyses of accelerometer data focus on the total activity over the day.** Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r}
total_activity = 
  accel %>% 
  group_by(week, day_id,day, weekday_weekend) %>% 
  summarise(total_activity = sum(activity_counts)) %>% 
  arrange(week, weekday_weekend)

knitr::kable(total_activity)
```

I arranged week and weekday_weekend variable but I did not find some significent trend. I do find that there might be a slightly decreasing in weekend total activity so I calculate the mean avtivity counts within each week to find if there is any trend of the toal acticity for the 5 weeks. 

```{r}
mean_activitiy_within_a_week = 
  accel %>% 
  group_by(week) %>% 
  summarise(mean_activity = mean(activity_counts))

knitr::kable(mean_activitiy_within_a_week)
```

It seems that there is a decrease but not very clear.

## Make a single-panel plot

Shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

```{r}
plot =   
  accel %>% 
  mutate(
    day = fct_relevel(as_factor(day), "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"),
    activity = as.numeric(activity)
    )%>% 
  group_by(week, day) %>% 
  ggplot(aes(x = activity, y = activity_counts, color = day))+
  geom_line()+
  labs(
    x = "minutes",
    y = "activity counts",
    title = "24-hour activity time courses for each day"
  )+
  theme(plot.title = element_text(hjust = 0.5))

plot
```

From the plot we can see that, at noon on Sunday and late at night on each day, the activity counts are much higher than midnight and afternoon each day. 