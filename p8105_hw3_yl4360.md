p8105\_HW3\_yl4360
================
Yiyang Liu

``` r
data("instacart")
data("brfss_smart2010")
```

# Problem 1

## Short discription

  - **The size and structure of the data:** there are 1384617
    observations and 15 variables in the data. 11 variables are integer
    and 4 of them are character.

  - **Describing some key variables:**
    
      - `reordered`: 1 if this prodcut has been ordered by this user in
        the past, 0 otherwise
      - `order_dow`: the day of the week on which the order was placed
      - `eval_set`: which evaluation set this order belongs in *(Note
        that the data for use in this class is exclusively from the
        “train” `eval_set`)*

  - **Giving illustrative examples of observations:** For the 1st
    observation, order id is 1. It was ordered at 10 on Thursday and it
    is reordered 9 days after the prior order. The product name is
    Bulgarian Yogurt from department dairy eggs. The 1st order consists
    of 8 products. User 112108 ordered them.

<!-- end list -->

``` r
aisle_level = 
  pull(instacart, aisle) %>% 
  as.factor() %>% 
  levels()
head(aisle_level)
```

    ## [1] "air fresheners candles" "asian foods"           
    ## [3] "baby accessories"       "baby bath body care"   
    ## [5] "baby food formula"      "bakery desserts"

``` r
aisle_max = 
  instacart %>%
  group_by(aisle) %>%
  summarise(number = n()) %>%
  filter(number == max(number)) %>%
  select(aisle) %>%
  as.character()
```

  - **The number of aisles and the aisle that the most items are ordered
    from**: there are 134 aisles and the aisle that the most items are
    ordered from is fresh vegetables.

## Plot 1

Showing the number of items ordered in each
aisle.

<img src="p8105_hw3_yl4360_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

According to the plot, it is hard to tell whether the orders from fresh
vegetables or the ones from fresh fruits is the most but the result from
coding is that the orders from fresh vegetables is the most. Therefore I
want to further verify.

``` r
aisle_max_ver = 
  instacart %>% 
  group_by(aisle) %>% 
  summarise(number = n()) %>% 
  arrange(desc(number)) %>% 
  head(2)
aisle_max_ver
```

    ## # A tibble: 2 x 2
    ##   aisle            number
    ##   <chr>             <int>
    ## 1 fresh vegetables 150609
    ## 2 fresh fruits     150473

It is verified that the orders from fresh vegetables is the most.

## Table 1

Showing the three most popular items in each of the aisles “baking
ingredients”, “dog food care”, and “packaged vegetables fruits”.

``` r
table1 = 
  instacart %>% 
  filter(aisle == "baking ingredients" | 
         aisle == "dog food care" | 
         aisle == "packaged vegetables fruits") %>%
  group_by(aisle, product_name) %>%
  summarize(n = n()) %>%
  head(3) %>%
  arrange(desc(n))
knitr::kable (table1)
```

| aisle              | product\_name                                |  n |
| :----------------- | :------------------------------------------- | -: |
| baking ingredients | 1 to 1 Gluten Free Baking Flour              | 11 |
| baking ingredients | 1-to-1 Baking Flour, Gluten/Wheat/Dairy Free |  7 |
| baking ingredients | 10" Graham Ready Crust                       |  5 |

## Table 2

Showing the mean hour of the day at which Pink Lady Apples and Coffee
Ice Cream are ordered on each day of the week.

``` r
instacart %>%
  select(product_name, order_hour_of_day, order_dow) %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  group_by(product_name, order_dow) %>%
  summarise(mean_hour = mean(order_hour_of_day)) %>%
  mutate(
    mean_hour = round(mean_hour,1),
    order_dow = recode(order_dow, "0" = "Sun", "1" = "Mon", "2" = "Tue", "3" = "Wed", "4" = "Thur", "5" = "Fri", "6" = "Sat")
  ) %>%
  pivot_wider(
    names_from = "order_dow",
    values_from = "mean_hour"
  ) %>%  
  knitr::kable()
```

| product\_name    |  Sun |  Mon |  Tue |  Wed | Thur |  Fri |  Sat |
| :--------------- | ---: | ---: | ---: | ---: | ---: | ---: | ---: |
| Coffee Ice Cream | 13.8 | 14.3 | 15.4 | 15.3 | 15.2 | 12.3 | 13.8 |
| Pink Lady Apples | 13.4 | 11.4 | 11.7 | 14.2 | 11.6 | 12.8 | 11.9 |

# Problem 2

  - **Clean the data.**

<!-- end list -->

``` r
brfss_clean = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename("state" = "locationabbr",
       "county" = "locationdesc") %>% 
  filter(
    topic == "Overall Health",
    response == "Excellent" | 
    response == "Very good" | 
    response == "Good"| 
    response == "Fair"|
    response == "Poor") %>% 
  mutate(response = factor(response, levels = c("Poor","Fair","Good","Very good", "Excellent"))) %>% 
   arrange(response)

brfss_clean
```

    ## # A tibble: 10,625 x 23
    ##     year state county class topic question response sample_size data_value
    ##    <int> <chr> <chr>  <chr> <chr> <chr>    <fct>          <int>      <dbl>
    ##  1  2010 AL    AL - ~ Heal~ Over~ How is ~ Poor              45        5.5
    ##  2  2010 AL    AL - ~ Heal~ Over~ How is ~ Poor              66        6.4
    ##  3  2010 AL    AL - ~ Heal~ Over~ How is ~ Poor              35        4.2
    ##  4  2010 AZ    AZ - ~ Heal~ Over~ How is ~ Poor              62        3.5
    ##  5  2010 AZ    AZ - ~ Heal~ Over~ How is ~ Poor              49        5.7
    ##  6  2010 AZ    AZ - ~ Heal~ Over~ How is ~ Poor              30        4  
    ##  7  2010 AR    AR - ~ Heal~ Over~ How is ~ Poor              21        3  
    ##  8  2010 AR    AR - ~ Heal~ Over~ How is ~ Poor              36        3.8
    ##  9  2010 AR    AR - ~ Heal~ Over~ How is ~ Poor              16        2.4
    ## 10  2010 CA    CA - ~ Heal~ Over~ How is ~ Poor              23        2.4
    ## # ... with 10,615 more rows, and 14 more variables:
    ## #   confidence_limit_low <dbl>, confidence_limit_high <dbl>,
    ## #   display_order <int>, data_value_unit <chr>, data_value_type <chr>,
    ## #   data_value_footnote_symbol <chr>, data_value_footnote <chr>,
    ## #   data_source <chr>, class_id <chr>, topic_id <chr>, location_id <chr>,
    ## #   question_id <chr>, respid <chr>, geo_location <chr>

  - **In 2002, which states were observed at 7 or more locations? What
    about in 2010?**

<!-- end list -->

``` r
states2002 = 
  brfss_clean %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n_location = n_distinct(county)) %>% 
  filter(n_location >= 7) 

states2010 = 
  brfss_clean %>% 
  filter(year == 2010) %>% 
  group_by(state) %>% 
  summarize(n_location = n_distinct(county)) %>% 
  filter(n_location >= 7) 
```

In 2002, CT, FL, MA, NC, NJ, PA were observed at 7 or more locations. In
2010, CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, WA were
observed at 7 or more locations.

## Construct a dataset and Make a “spaghetti” plot

Dataset: limited to Excellent responses, and contains, year, state, and
a variable that averages the data\_value across locations within a
state.

Plot: showing this average value over time within a state.

``` r
sp_plot = 
  brfss_clean %>% 
  filter(response == "Excellent") %>% 
  select(year, state, data_value) %>% 
  group_by(state, year) %>% 
  summarise(average = mean(data_value)) %>% 
  ggplot(aes(x = year, y= average, group = state))+
  geom_line(aes(color = state))+
  labs(
      title = "Spaghetti plot of average value over time",
      x = "Year",
      y = "Average data value"
    ) +
    theme(plot.title = element_text(hjust = 0.5))

sp_plot
```

<img src="p8105_hw3_yl4360_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />

The Spaghetti plot is very complicated but we can read that average data
value of most state is around 20\~25 over 8 years. There is one state
reached the lowest value in 2005.

## Make a two-panel plot

Showing, for the years 2006, and 2010, distribution of data\_value for
responses (“Poor” to “Excellent”) among locations in NY State.

``` r
tp_plot = 
  brfss_clean %>% 
  filter(
    year == 2006 | year == 2010,
    response == "Excellent" | 
    response == "Very good" | 
    response == "Good"| 
    response == "Fair"|
    response == "Poor",
    state == "NY") %>% 
  select(year, county, response, data_value) %>% 
  ggplot(aes(x = response, y = data_value, group = county, color = response))+
  geom_point(size = 3, alpha = .5) + 
  geom_line() +
  labs(
    title = "Distribution of data_value for responses in NY, 2006",
    x = "county",
    y = "data value"
  )+
  facet_grid(~year) + 
   viridis::scale_color_viridis(
    name = "County",
    discrete = TRUE) +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 60, hjust = 1))

tp_plot  
```

<img src="p8105_hw3_yl4360_files/figure-gfm/unnamed-chunk-10-1.png" width="90%" />

According to the two-panel plot, we can see that there are two more
counties, Bronx and Erie added into the data in 2010. Generally, “poor”
reponse matches with lowest data value and “Excellent” reponse match
with highest data value.

# Problem 3

  - **Load, tidy, and otherwise wrangle the data.** Your final dataset
    should include all originally observed variables and values; have
    useful variable names; include a weekday vs weekend variable; and
    encode data with reasonable variable classes. Describe the resulting
    dataset (e.g. what variables exist, how many observations, etc).

<!-- end list -->

``` r
accel_data = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
  activity_1:activity_1440,
  names_to = "activity",
  names_prefix = "activity_",
  values_to = "activity_counts"
  ) 
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

``` r
weekend =
  accel_data %>%
  filter(day == "Sunday" | day == "Saturday") %>%
  mutate( weekday_weekend = "weekend") %>%
  select(week, day_id, day, weekday_weekend, everything())

weekday = 
  accel_data %>%
  filter(day == "Monday" | day == "Tuesday" | day == "Wednesday" | day == "Thursday" | day == "Friday") %>%
  mutate( weekday_weekend = "weekday") %>%
  select(week, day_id, day, weekday_weekend, everything())

accel =
  merge(weekend, weekday, all = TRUE) %>%
  arrange(week, day_id) 
```

There are 50400 observations of 5 variables in the dataset. The
variables are week, day\_id, day, activity, activity\_counts.

  - **Traditional analyses of accelerometer data focus on the total
    activity over the day.** Using your tidied dataset, aggregate
    accross minutes to create a total activity variable for each day,
    and create a table showing these totals. Are any trends apparent?

<!-- end list -->

``` r
total_activity = 
  accel %>% 
  group_by(week, day_id,day, weekday_weekend) %>% 
  summarise(total_activity = sum(activity_counts)) %>% 
  arrange(week, weekday_weekend)

knitr::kable(total_activity)
```

| week | day\_id | day       | weekday\_weekend | total\_activity |
| ---: | ------: | :-------- | :--------------- | --------------: |
|    1 |       1 | Friday    | weekday          |       480542.62 |
|    1 |       2 | Monday    | weekday          |        78828.07 |
|    1 |       5 | Thursday  | weekday          |       355923.64 |
|    1 |       6 | Tuesday   | weekday          |       307094.24 |
|    1 |       7 | Wednesday | weekday          |       340115.01 |
|    1 |       3 | Saturday  | weekend          |       376254.00 |
|    1 |       4 | Sunday    | weekend          |       631105.00 |
|    2 |       8 | Friday    | weekday          |       568839.00 |
|    2 |       9 | Monday    | weekday          |       295431.00 |
|    2 |      12 | Thursday  | weekday          |       474048.00 |
|    2 |      13 | Tuesday   | weekday          |       423245.00 |
|    2 |      14 | Wednesday | weekday          |       440962.00 |
|    2 |      10 | Saturday  | weekend          |       607175.00 |
|    2 |      11 | Sunday    | weekend          |       422018.00 |
|    3 |      15 | Friday    | weekday          |       467420.00 |
|    3 |      16 | Monday    | weekday          |       685910.00 |
|    3 |      19 | Thursday  | weekday          |       371230.00 |
|    3 |      20 | Tuesday   | weekday          |       381507.00 |
|    3 |      21 | Wednesday | weekday          |       468869.00 |
|    3 |      17 | Saturday  | weekend          |       382928.00 |
|    3 |      18 | Sunday    | weekend          |       467052.00 |
|    4 |      22 | Friday    | weekday          |       154049.00 |
|    4 |      23 | Monday    | weekday          |       409450.00 |
|    4 |      26 | Thursday  | weekday          |       340291.00 |
|    4 |      27 | Tuesday   | weekday          |       319568.00 |
|    4 |      28 | Wednesday | weekday          |       434460.00 |
|    4 |      24 | Saturday  | weekend          |         1440.00 |
|    4 |      25 | Sunday    | weekend          |       260617.00 |
|    5 |      29 | Friday    | weekday          |       620860.00 |
|    5 |      30 | Monday    | weekday          |       389080.00 |
|    5 |      33 | Thursday  | weekday          |       549658.00 |
|    5 |      34 | Tuesday   | weekday          |       367824.00 |
|    5 |      35 | Wednesday | weekday          |       445366.00 |
|    5 |      31 | Saturday  | weekend          |         1440.00 |
|    5 |      32 | Sunday    | weekend          |       138421.00 |

I arranged week and weekday\_weekend variable but I did not find some
significent trend. I do find that there might be a slightly decreasing
in weekend total activity so I calculate the mean avtivity counts within
each week to find if there is any trend of the toal acticity for the 5
weeks.

``` r
mean_activitiy_within_a_week = 
  accel %>% 
  group_by(week) %>% 
  summarise(mean_activity = mean(activity_counts))

knitr::kable(mean_activitiy_within_a_week)
```

| week | mean\_activity |
| ---: | -------------: |
|    1 |       254.9467 |
|    2 |       320.6069 |
|    3 |       319.9321 |
|    4 |       190.4638 |
|    5 |       249.2707 |

It seems that there is a decrease but not very clear.

## Make a single-panel plot

Shows the 24-hour activity time courses for each day and use color to
indicate day of the week. Describe in words any patterns or conclusions
you can make based on this graph.

``` r
plot =   
  accel %>% 
  mutate(
    day = fct_relevel(as_factor(day), "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"),
    activity = as.numeric(activity)
    )%>% 
  group_by(week, day) %>% 
  ggplot(aes(x = activity, y = activity_counts, color = day))+
  geom_line()+
  labs(
    x = "minutes",
    y = "activity counts",
    title = "24-hour activity time courses for each day"
  )+
  theme(plot.title = element_text(hjust = 0.5))

plot
```

<img src="p8105_hw3_yl4360_files/figure-gfm/unnamed-chunk-14-1.png" width="90%" />

From the plot we can see that, at noon on Sunday and late at night on
each day, the activity counts are much higher than midnight and
afternoon each day.
